{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce293c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "os.chdir(\"/root/autodl-tmp/SCALED-Particle\")\n",
    "\n",
    "\n",
    "# pred_files = sorted(glob.glob(\"exp/04_attn_sfc/rollout/npy/*.npy\"))\n",
    "# pred_files = sorted(glob.glob(\"exp/03_conv_sfc/rollout/npy/*.npy\"))\n",
    "# pred_files = sorted(glob.glob(\"exp/02_attn/rollout/npy/*.npy\"))\n",
    "pred_files = sorted(glob.glob(\"exp/01_conv/rollout/npy/*.npy\"))\n",
    "pred_arrays = [np.load(f) for f in pred_files]  # shape (8,256,64,64)\n",
    "\n",
    "gt_files = sorted(glob.glob(\"data/evaluation_npy_step200-250/*.npy\"))\n",
    "gt_arrays = [np.load(f) for f in gt_files]  # shape (8,256,64,64)\n",
    "\n",
    "\n",
    "def analyze_timesteps(gt_arrays, pred_arrays, output_csv=\"result/stat/stat.csv\"):\n",
    "    \"\"\"\n",
    "    Calculate statistics for each timestep of ground truth and prediction arrays\n",
    "    Outputs a CSV file with rows containing gt, pred, and error metrics (diff, rel_err)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # subdomain index\n",
    "    x_start, x_end = 64//2 - 64 //4, 64//2 + 64 //4\n",
    "    y_start, y_end = 64//2 - 64 //4, 64//2 + 64 //4\n",
    "    z_start, z_end = 256//2 - 256 //4, 256//2 + 256 //4\n",
    "\n",
    "    def compute_metrics(arr):\n",
    "        \"\"\"Calculate various metrics for a single timestep\"\"\"\n",
    "        particle_vel = arr[0:3]  # (3, 256, 64, 64)\n",
    "        fluid_vel = arr[3:6]     # (3, 256, 64, 64)\n",
    "        mask = arr[7] > 0        # boolean mask\n",
    "\n",
    "        # 1. particle velocity mean\n",
    "        particle_vel_avg = []\n",
    "        for i in range(3):\n",
    "            vals = particle_vel[i][mask]\n",
    "            particle_vel_avg.append(vals.mean() if vals.size > 0 else np.nan)\n",
    "\n",
    "        # 2. fluid velocity mean\n",
    "        fluid_vel_avg = [fluid_vel[i].mean() for i in range(3)]\n",
    "\n",
    "        # 3. Reynolds stress\n",
    "        u_fluct = [fluid_vel[i] - fluid_vel_avg[i] for i in range(3)]\n",
    "        R_xx = np.mean(u_fluct[0] * u_fluct[0])\n",
    "        R_yy = np.mean(u_fluct[1] * u_fluct[1])\n",
    "        R_zz = np.mean(u_fluct[2] * u_fluct[2])\n",
    "        R_xy = np.mean(u_fluct[0] * u_fluct[1])\n",
    "        R_xz = np.mean(u_fluct[0] * u_fluct[2])\n",
    "        R_yz = np.mean(u_fluct[1] * u_fluct[2])\n",
    "\n",
    "        # 4. particle volume fraction (subdomain)\n",
    "        sub_mask = mask[z_start:z_end, x_start:x_end, y_start:y_end]  # (256, 60, 60)\n",
    "        vol_frac_per_layer = sub_mask.mean(axis=(1, 2))\n",
    "        vol_frac_mean = vol_frac_per_layer.mean()\n",
    "        vol_frac_std = vol_frac_per_layer.std()\n",
    "\n",
    "        return {\n",
    "            \"particle_vel_avg_x\": particle_vel_avg[0],\n",
    "            \"particle_vel_avg_y\": particle_vel_avg[1],\n",
    "            \"particle_vel_avg_z\": particle_vel_avg[2],\n",
    "            \"fluid_vel_avg_x\": fluid_vel_avg[0],\n",
    "            \"fluid_vel_avg_y\": fluid_vel_avg[1],\n",
    "            \"fluid_vel_avg_z\": fluid_vel_avg[2],\n",
    "            \"fluid_Re_xx\": R_xx,\n",
    "            \"fluid_Re_yy\": R_yy,\n",
    "            \"fluid_Re_zz\": R_zz,\n",
    "            \"fluid_Re_xy\": R_xy,\n",
    "            \"fluid_Re_xz\": R_xz,\n",
    "            \"fluid_Re_yz\": R_yz,\n",
    "            \"particle_vol_frac_mean\": vol_frac_mean,\n",
    "            \"particle_vol_frac_std\": vol_frac_std\n",
    "        }\n",
    "\n",
    "    eps = 1e-8  # prevent division by zero\n",
    "\n",
    "    # iterate over timesteps (assuming gt and pred have the same number of timesteps)\n",
    "    for t_idx, (gt_arr, pred_arr) in enumerate(zip(gt_arrays, pred_arrays)):\n",
    "        gt_metrics = compute_metrics(gt_arr)\n",
    "        pred_metrics = compute_metrics(pred_arr)\n",
    "\n",
    "        row = {\"timestep\": t_idx}\n",
    "        # GT and Pred\n",
    "        for k in gt_metrics:\n",
    "            row[f\"{k}_gt\"] = gt_metrics[k]\n",
    "            row[f\"{k}_pred\"] = pred_metrics[k]\n",
    "            # error: difference & relative error\n",
    "            row[f\"{k}_diff\"] = np.abs(pred_metrics[k] - gt_metrics[k])\n",
    "            row[f\"{k}_relerr\"] = np.abs(pred_metrics[k] - gt_metrics[k]) / (np.abs(gt_metrics[k]) + eps)\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved results to {output_csv}\")\n",
    "    return df\n",
    "\n",
    "def summarize_errors(df, output_csv=\"comparison_summary.csv\"):\n",
    "    \"\"\"\n",
    "    Input a DataFrame of error metrics per timestep, output mean/max/std of each metric\n",
    "    \"\"\"\n",
    "    error_cols = [c for c in df.columns if c.endswith(\"_diff\") or c.endswith(\"_relerr\")]\n",
    "    summary = {}\n",
    "\n",
    "    for col in error_cols:\n",
    "        summary[col] = {\n",
    "            \"mean\": df[col].mean(),\n",
    "            \"max\": df[col].max(),\n",
    "            \"std\": df[col].std()\n",
    "        }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary).T  # row: metric, column: mean/max/std\n",
    "    summary_df.to_csv(output_csv)\n",
    "    print(f\"Saved summary to {output_csv}\")\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# df_detailed = analyze_timesteps(gt_arrays, pred_arrays)\n",
    "# os.makedirs(\"result/stat\", exist_ok=True)\n",
    "# df_summary = summarize_errors(df_detailed, output_csv=\"result/stat/stat_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d79c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "methods = {\n",
    "    \"conv\": \"exp/01_conv/rollout/npy/*.npy\",\n",
    "    \"attn\": \"exp/02_attn/rollout/npy/*.npy\",\n",
    "    \"conv_sfc\": \"exp/03_conv_sfc/rollout/npy/*.npy\",\n",
    "    \"attn_sfc\": \"exp/04_attn_sfc/rollout/npy/*.npy\",\n",
    "}\n",
    "\n",
    "gt_files = sorted(glob.glob(\"data/evaluation_npy_step200-250/*.npy\"))\n",
    "gt_arrays = [np.load(f) for f in gt_files]\n",
    "\n",
    "# for each method, generate summary csv\n",
    "summary_dfs = {}\n",
    "for method_name, pattern in methods.items():\n",
    "    pred_files = sorted(glob.glob(pattern))\n",
    "    pred_arrays = [np.load(f) for f in pred_files]\n",
    "    \n",
    "    df_detailed = analyze_timesteps(gt_arrays, pred_arrays, output_csv=f\"result/stat/{method_name}_detailed.csv\")\n",
    "    df_summary = summarize_errors(df_detailed, output_csv=f\"result/stat/{method_name}_summary.csv\")\n",
    "    summary_dfs[method_name] = df_summary\n",
    "\n",
    "# -----------------------------\n",
    "# plot multi-method bar chart\n",
    "# -----------------------------\n",
    "def plot_multi_method(summary_dfs, metrics=None, metric_type=\"diff\", use_std=False, logscale=False):\n",
    "    \"\"\"\n",
    "    summary_dfs: dict, {method_name: summary_df}\n",
    "    metrics: list, specify metrics to plot (excluding _diff/_relerr)\n",
    "    metric_type: \"diff\" or \"relerr\"\n",
    "    use_std: whether to plot std\n",
    "    \"\"\"\n",
    "    method_names = list(summary_dfs.keys())\n",
    "    n_methods = len(method_names)\n",
    "    \n",
    "    # default to plot all metrics\n",
    "    if metrics is None:\n",
    "        metrics = [c.replace(f\"_{metric_type}\",\"\") for c in summary_dfs[method_names[0]].index if metric_type in c]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.18  # width of each bar\n",
    "    fig, ax = plt.subplots(figsize=(16,6))\n",
    "    \n",
    "    colors = [\"skyblue\", \"salmon\", \"limegreen\", \"orange\"][:n_methods]\n",
    "    \n",
    "    for i, method in enumerate(method_names):\n",
    "        df = summary_dfs[method]\n",
    "        col_names = [f\"{m}_{metric_type}\" for m in metrics]\n",
    "        means = df.loc[col_names, \"mean\"].values\n",
    "        if use_std:\n",
    "            errs = df.loc[col_names, \"std\"].values\n",
    "        else:\n",
    "            errs = None\n",
    "        ax.bar(x + i*width, means, width, yerr=errs, capsize=3, label=method, color=colors[i])\n",
    "    \n",
    "    if logscale:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_xticks(x + width*(n_methods-1)/2)\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Absolute Difference (pred - gt)\" if metric_type==\"diff\" else \"Relative Error\")\n",
    "    ax.set_title(f\"Comparison of {metric_type} Across Methods\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"result/stat/multi_method_{metric_type}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# plot diff\n",
    "plot_multi_method(summary_dfs, metric_type=\"diff\", use_std=False)\n",
    "\n",
    "# plot relative error\n",
    "# plot_multi_method(summary_dfs, metric_type=\"relerr\", use_std=False, logscale=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
